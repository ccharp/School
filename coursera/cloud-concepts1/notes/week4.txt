KEY-VALUE STORES
- Twitter uses them for tweets
    -distrubted hash table
        - for when you have too much data to store
- problem with relational database
    - problems are large and often unstructured
    - a lot of writes compared to reads
    - foreign keys and joins are not often needed in today's work-loads
- needs of today:
    - speed
    - avoid Single Point of Falure (SPoF)
    - minimize Total Cost of Operation (TCO)
    - incremental scalability
    - "scale out, not up"
        - scale up: grow cluster by replacing with more powerful machines
            - not cost effective
            - need to replace often
        - scale out: grow cluster by adding more COTS machines (Components Off the Shelf)
            - cheaper
            - over long duration, phose in few newer machines and phase out old ones
            - used by most companies 
- NoSQL
    - "Not Only SQL"
    - necessary API operations: get(key), put(key, value)
    - Tables
        - Cassandra calls them "Column Families"
        - MongoDB: collection
        - like RDBMS but
            - may be unstructured (no schema imposed)
            - might have missing columns
            - no foreign keys or joins (usually)
- Column-oriented storage
    - RDBMSs store entire row together
    - NoSQL systems typically store columns together (or group of columns)
    - faster and don't need to fetch entire table
    - eg get blob_ids from blog table in the past month
        - search in the last_updated column, fetch corresponding blog_id column
        - don't need to fetch other columns
Apache Cassandra
- distrubted key-value store
- originally designed at Facebook for messaging
- inside Cassandra: 
    - uses ring from DHT
    - client sends query to Coordinator
    - one ring present per data center
    - no finger tables!
    - coordinated forwards query to appropriate replicas of key
    - mapping from key to server
        - handled by partitioner
        - Data Placement Strategies
            - Simple Strategy
                - Random Partitioner: chord-like hash partitioning
                - ByteOrdered Partitioner
                    - assignes ranges of keys to servers
                    - useful for "range queries" (get all twitter users starting with [a-b]
            - Network Topology Strategy
                - two replica's per DC
                - three replica's per DC
                - first replica placed according to Partitioner
                - go around ring until you hit a different rack
                    - want keys on different rack (fault tolerance)
    - Snithces: map IPs to racks and DCs
        - SimpleSnitch: unawayre of toplogy 
        - rackInferring: assumes topology of network bo octet of server's IP address
            - 101.102.103.104 = x.<DC octet>.<rack_octet>.<node octet>
        - PropertyFileSnitch: perfectly accurate. uses config file
        - EC2Snitch: EC2 Region = DC, Availabilit Zone = rack
    - writes and reads
        - writes need to be lock free and fast
        - client sends write to one coordinator node in Cassandra cluster
            - coordinator may be per-key, per-client, or per-query (what?)
            - coordinator uses partitioner to send query to all replica nodes respnosible for key
            - when X replicas respond, coordinate returns ack to lcient
        - keys are always writable (no locking, not down)
            - Hinted Handoff:
                - when any replica is down, coordinator writes to all other replicas and keeps it locally until down replicas come back up
                - when all replicas are down, coordinator buffers writes locally for a few hours
        - one wring per data centeR:
            - DC coordinator elected to coordinate with other DCs
            - osmething about a zoo keeper
        - what does replica server to when received a write?
            - log it to disk commit log (for failure recovery)
            - make changes to memtables
                - doesn't write directly to disk for speed
            - when memtable is full or old, it's flushed to disk
                - SSTable (key, position of data in sstable) of pairs
                - Bloom filter for efficient search
                    - easy to check to existence
                    - can instert
                    - can have false positives, but not false negatives
                    - basically it's a bitmap. each key maps to set of bits in the bitmap.
                        - possible that key was never instereted (part of bits set by some key, othe rparts set by another)
                        - however, this is very low with 4 hashing functions (0.02%)
                        - can be further mitigated by adding more bits to bloom table
                - delete
        - read is very similar to write
            - coordinator contacts X replicas
            - when replica's resond, coordinate returns latest-timestammepd value 
            - if replicas a different, initiate a "read repair"
            - row may be split across multiple SSTables
                - reads may need to touch multiple SSTables, therefore reads usually slower than writes, but still fast!
                - this happens if compaction hasn't happened often enough
    - mempership
        - every server needs to know about all other servers present in cluster
        - gossip style membership lists :D
        - suspicion mechanisms to adaptively timing out 
        - accrual detector: uses PHI output
            - PHI: looks at last arrival time and other arrivals (probabalistic)
                - historically slower servers have longer timtouts
                - faster servers have shorter timeouts
- spped?
    - about 10x faster than MySQL
    - what did we lose?
         - discussed in next lecture.
Mystery of X:
- CAP Theorem
    - 3 properties of distributed system
        - 1. consistency: all nodes see same data at same time
            - think accessing bank from multiple clients (mobile, pc, ect.)
            - booking a flight: all clients should see same seats as they're available
        - 2. availability: everything works all the time and quickly
            - high latency means less revenue
            - Service Level Agreements: contract for latencies basically
        - 3. partition-tolerance: system continues to work inspite of network partitions
            - DNS outtages
            - routing outages
            - undersea cable cut
            - etc. 
    - only 2 of these 3 things are possible at a time
    - Partition-tolerance is usually a given, so we're choosing between consistency and availability
    - Cassandra goes for availability
    - RDBMSs go for consistency
- eventual consistency
    - given a key, suppose all writes to that key stop
        - all values (replicas) will converge eventually
        - if writes continue, system will continue trying to converge
        - works well when periods of low writes
            - not so well of a lot of consecutive writes
- RDBMS ACID
    - Atomicyt, Consistency, Isolaction, Durability
- Key-vlaue stores BASE
    - Basically Available Soft-state Eventual Consistent
- so how do we specify the value of X?
    - X is a Consistency Level
    - can be:
        - ANY: any server may be replica
            - fastest. even allows coordinated to store value before returning ACK to client
        - ALL: all replicas
            - ensures all replicas are consistent. Strongest cinsistency
        - ONE: at least one replica
            - faster than ALL, but cannot tolerate a failure
        - QUORUM: quorum across al replicas in datacenter
    - what is a quorum?
        - majority, > 50%
            -therefore, at least one server from a different rack will be in quorum
        - don't require all replicas to return ACK
            - faster than ALL but still strongly consistent
Consistency Spectrum
    tradeof:
- consistency spectrum: Eventual -> strong
- read writes:          slow      <- fast
- per-key sequential: per key, all operations have a global order (per key coordinator)
    - CRDTs (Commutative REplicated Data Types): reversing order gives same result
        - eventually, servers don't need to worry about consistency
    - red-consistency: rewrite client transatcionts int red or blue operations
        - blue ops can be executed in any order
        - red ops need to be executed in order
    - Causal Consistency: reads must respect partial order based on imformation flow (what?)
        - reads obeys causality (cause path from previous writes by same user, for example)
- Stong Consistency Models
    - Lineraizability: each operation by  aclient is visislbe instantaneously (hard to do, ovbiously)
    - sequential consistency: re-ording obeys causality, writes before reads, etc.
    - ACID properties in NewSQL stuff. (Google's Tracker)
NoSQL: HBase
- Yahoo's opensource project of Google's BigTable
- API functions: 
    - Get, Put
    - Scan(row range, filter) (all user names, etc)
    - prefers consistency over available
- architecture:
    - zookeeper coordinates client and server
    - HRegionServer
        - hspace table split across regions
            - column family
            - for every column family within a region, maintain a store and memstore    
                - these stores are flushed to disk every so often
                    - HFile is an SSTable
- how does it maintain strong consistency?
    - HLog
        - written to before writing to MemStore to recover from failure
        - replays logs when recovery or booting up
    - cross-datacenter replication
        - master and slave DCs
            - master sends logs to slaves
            - coordination done by zookeeper
Time and Ordering
- don't want to be early or late, perfectly sychronized
- clocks have to be in sync, local and server
- events in a process can be timestamed and thus can be ordered linearly
- clock scew: relative difference in clock VALUES of two processes
- clock drift: relatie difference between SPEED of two processes
- MDR (maximum drift rate)
    - maximum drift rate = MDR * 2
    - Maximum acceptible skew M, need to synch clocks every M / (2 * MDR)
- Extrenal and interal Synchronization
    - external: each processes's clock is within bound D of a well-known external group
    - internal: every processes in group is within bound D
Cristian's Algorithm
- uses RTT (round trip time)
- Cristian's Algorithm works between a process P, and a time server S — connected to a source of UTC (Coordinated Universal Time). Put simply:
    1. P requests the time from S
    2. After receiving the request from P, S prepares a response and appends the time T from its own clock.
    3. P then sets its time to be T + RTT/2
Time and Ordering
- NTP - network time protocol, used to synchronize
- each client is a leafe node
- primary and tertiary servers
- how does it work?
    - child wants to synch clock
    - parent responds with with time
    - child sends time
    - parent again sends time
    - child uses these values to set clock (multiple times reduces error
        - offset o = (tr1 - tr2 + tr2 - tl2)/2
        - thus error is bounded by round-trip time
        - BUT, we still have error....
Lamport Timestamps
- instead of tryin to synch, just put them in the right order- instead of tryin to synch, just put them in the right order 
    - uses causality as above
- almost all distributed systems like this
- how does it work?
    - three rules: 1) if events a and b are on the same process, if time(a) < time(b), b happend before
                   2) if p1 sends message to p2: send came before the receive
                   3) transitivity, extends rule 2. a->b, b->c then a->c
    - but not all events are related
- so how are time stamps actually assigned?
    - use a local counter, starts at zero for each processes (local counter)
    - whenever processes sends message, counter is incremented
    - sent message also carries timestamp
        - used by receiving timestamp, looks at local value, takes max, and adds 1
Vector Clocks
- yet another way of assigning timestamps
- each process maintains a vector
- like Lamport clock, 1) increments ith element of vector clock
                      2) each message carries the send event's vector timestamp V[1..N]
                      3) receiver takes of incoming timestamp and takes mach of each i and increments them by one








