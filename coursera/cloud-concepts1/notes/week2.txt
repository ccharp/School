Week 2: Gossip
Multicast Problem:
    - Multicast: want to get information from single node to other computers. (think PC talking to NY stock exchange
    - Requires FALT-TOLERANCE and SCALABILITYlk
    - multicast protocal often sits at application layer
    - Centralized Approach
        - UDP/TCP packet -- simplest implimentation
        - problems
            - if sender fails halfway sending info to all nodes, many nodes will not receive info
            - large burden on sender (resonsible for sending imfornation to all nodes
    - Tree-Based
        - UDP/TCP packet 
        - tree can remain balanced. 
        - each node passes message along to adjacdnet nodes
        - Problem: 
            - need to set up and maintain the tree
            - falures near root of tree can cause large number of nodes to not receive message
        - build spanning tree among the processes of multicast group
        - use ACKs or NACKs to repaire multicasts not received
        - SRM (Scalable Reliable Multicast) -- uses NACKS
        - RMTP (Reliable Multicast Transport Protocol) -- uses ACKS
            - re-transmit messages not ACKed
        - still have O(N) scalable
The Gossip Proticol - 
    - PUSH gossip proticol
        - periodically send random messages to b random targets (perhaps ever 2 seconds)
            - b is called "gossibe fanout"
            - gossip message is UDP
        - infected nodes: term used to describe node that received gossip message
            - they, in turn, send out the message to random nodes
            - nodes can receive message multiple times (waste)
        - Epidemic multicast (a.k.a. gossip)
    - PULL gossip proticol
        - instead of sending out gossips only when infected, send messages out all the time
            - the messages are queries, "hey, have you received any new messages lately?"
    - periodically send random messages to b random targets (perhaps ever 2 seconds)
        - b is called "gossibe fanout"
        - gossip message is UDP
    - infected nodes: term used to describe node that received gossip message
        - they, in turn, send out the message to random nodes
        - nodes can receive message multiple times (waste)
    - Epidemic multicast (a.k.a. gossip)
Gossip Analysis (push gossip)
    - Claimes: 1. is light weight
               2. spreads message quickly
               3. highly fault tolerant
    - derived from study of epidemiology (Baily)
    - should be covered in log(branching-factor) number of rounds
        - highly reliable, low latency
        - overhead on each node is alsy logatrhmic 
        - this is okay because log(all IPv4 addresses) is 32.
    - Fault-tolerance
        - Packet loss?
            - 0% packet loss only takes twice the number of rounds
        - Fault-tolerance
    - after about half the nodes have received the message PULL is faster than PUSH
        - much, much faster
    - these protocols are not topological aware
        - could overload router
        - Problem: suppose two subnets separated by router.
            - about half messages will go accross router (remember, ramdom)
            - fix: after subnet has been completed infected go accross subnet
Gossip Implementation
    - nothing interesting
Group Membership
    - Scenario: data center operator says there's not faiulres in data center
        - in data centers, failurs are the norm--not the exception
        - do you believe him? build a failure detecgtor
            - perhaps write a distributed failure detection system?
    - building a failure detection/correction system:
        - group of processes
        - membership list that contains list of all processes that have not failed
            - accessed by many applications, perhaps gossip application
            - membership protpcol
            - disctinction between failure and leave
            - list can be maintained by all processes complete, almost complete, or partially complete
                - strongly consistent: complete list
                - almost-complete: gossip style, SWIM, ...
                - partial-random: SCAMP, T-MAN, Cyclon, ...
            - dissemination: gives info about failure and maybe tries to correct
            - each member as a list
            - when process P_j crashes, failure detector component of "nearby" processes uses its
              dissimenation component to tell all other processes about the failur.
            - each process has it's own membership list
Gossip-style failure detection
    - each node maintains a list of heartbeats and times for all other nodes
        - local time (nodes could be in different time zones)
    - periodically sends membership list to Gossip style pass nodes
        - this way, heartbeats don't have to be sent to all other nodes
        - lists are merged
    - if beat isn't heard in T secnds, wait another T seconds (cleanup) before deleting that node from list
        - why? all nodes will have needed to recognized the fail
            - otherwise, ping-pong effect
    - hearbet takes logn time to propagate (because it uses gossip) 
Best failure detection scheme? (optimal)
    - proof stuff on slides
Another Probabilistic Failure Detection Scheme
    - SWIM (Scalable weakly-consistent infection-style m?)
    - no hearbeats 
    - protocol period = T time units
    - node pi tries to ping pj
        - if it recievs an ack, it's satisfied for the period
        - otherwise, it tries to ping in indirectly through another node (good visualization on slides)
    - how does it compare to hearbeat?
        - SWIM has constant detection and constant process load
    - many failurs in system increase likelyhood of mistake
    - round robin ping with random permutation of ping list after each traversal











