P2P Systems

Why Study P2P systems?
    - P2P techniques pervasive in cloud computing systems
    - key-value stores use Chord P2P hasing
Napster
    - 25% out of Madison Wisconsin was Napster
    - Napster is now an open protocol
    - each Napster client is called a peer
    - files stored by peers at peer local
    - filename is key to info about at napster server (lcation, etc.)
        - server does not store any files
    - all communication uses TCP
    - problems
        - servers are centralized point of congestion
        - no security, plaintext messages and passwords
Gnutella
    - eliminates servers
    -clients themselves act as servers, take on responsibilities
        - called servents
        - peers store peer pointers of neighbors
        - forms network graph overlaid on internet (subgraph of internet)
    - searching
        - query, query hit, ping, pong
        - query: contains minimum speed, search criteria
            - only goes as far as TTL is no zero (TTL decremented at each neighbor as it transmits)
                - ensures message des not circulate forever
        - query hit: contains num hits, port, ip, speed, file info, servent_id
            - query hits are reverse routed. follows path back to peer
                - each peer remembers where query came from
            - uses HTTP to transfer files
                - GET
            - if responding peer is behind firell
                - can prevent messages from coming in
                - allows anything going out
                - if GET fails, remote peer sends PUSH back to requesting peer so that they can issue outgoing message
                - if both are behind a firewall, Gnutella gives up
        - ping: used to get to know neighbors, update membership list
        - pong: respnose to ping
            - ping and pong are needed because neighbors are constantly coming and going
            - continuously refresh membership list
    - problems
        - ping and pong consume > 50% of traffic
            - multiplex: cache to reduce frequency
        - modem-connected hosts do not have enough bandwidth for passong Gnutella traffic
            - use proxy server for such peers 
        - freeloaders: 70% of users that only download files, do not serve out files
        - why can't we intelligently send out messages so they don't hit ever node in the system?
            - solution: "structured" P2P systems
FastTrack and BitTorrent
    - like combination of Gnutella and Napster. 
    - uses supernodes, nodes that have historically contributed the most and are fastest
    - BitTorrent addresses lack of contribution by giving incentives
        - has a series of trackers for each file
            -maintains list of peers transferring the file
                - gets heartbeats from peers
        - two kinds of peers
            - seeds
            - leechers
                - has some files, looking to download other blocks (can contribute some)
        -file is split into blocks (usually around 256kb)
        - uses Local Rarest First polcy to choose block to download
            - local rarest meaning rarest among neighbors.
        - tit for tat bandwidth usage
            - provide blocks to users that provided good downloads in the past
                - incentive for contributing
        - choking limits number of neighbors for uploads going on
            - periodically unchoke new neighbor (re-evaluate every few seconds)
Chord
    - Distributed Hash Table (DHT)
        - store objects in nodes/machines in cluster
        - problems
            - load balancing. want want amout of stuff in each bucket (node)
            - fault tolerance, nodes can fail
            - efficiency and locality
        - Napster, Gnutella, Fastrack, are all kind of DHTs
            -but they don't really try to optimize regular hash operations (instert, etc)
        - Chord tries to be more like a DHC
            - Napster memory (n), loopup (1), messages for lookup (1)
            - Gnutella memory(n), (n), (n)
            - chord makes all operations log(n)
                - this is almost constant accross IPv4 addresses (32)
    - each node selects neighbors in intelligent fashion
        - opposed to Gnutella that just used #files shared or something
    - peer pointers: (used to route queries). places nodes on the rings
        - Consistent Hashing    
            - SHA-1 on IP address -> 160 bit string
            - truncate to m bits called peer ID (number between 0-2^m - a)
            - not unique but conflicts are very, very unlikely
            - nodes are arranged in ring, each node knows it's successor (ascending IDs)
        - Finger Tables
            - ith entry at peer with id n is first peer with id >= ((n + 2^i) mod (2^m))
            - look at slide 10 minutes in
    - how are files placed?
        -apply SHA-1 on file name
        - map file to point in a ring, stored at first peer
    - chord implementations can be found in many key-value pair DBs
Chord Falure Detection:
    - when nodes fail, lookups may go wrong
    - replicate files incase node fails
    - churn: rate at which nodes leave and join system
        - need to update ring and finger table entries
        - could leave to excessive copying
    - stabilization proticol taksl n^2 rounds
P2P Systems: Pastry
    - assignes IDs to nodes using consistent hasing function (like chord)
    - maintains neighbors a little differently
        - uses prefixing mapping instead of formula above (Hypercube routing), log(n)
        - short hops to neighbors
    - m bit id means m neighbors stoed
        - ex id: 01110100101
            - store *, 0*, 01*, 011*
            - match largest matching prefix
            - clostest candidate used if collision with id prefix
    - possible to design system with O(1) lookup costs?
P2P Systems: Kelips
    - consant lookup cost to destributed hashtable
    - no virtual ring
    - k affinit groups
        - k ~ sqrt(n)
        - k groups
    - every peer belongs to a group
        - each peer knows about every other peer in the group
        - each group contains about k nodes (K*K=n)
    - loopup:
        - hash to get gruop #
        - contact neigh of necessary
    - updating neighbors
        - gossip protocols (week 2 lectures)
    - uses slgithly more bandwidth to keep things maintained
    tradeoffs with Chord and Pastry
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
